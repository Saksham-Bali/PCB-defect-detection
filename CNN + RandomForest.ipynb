{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjR92m_IifSm",
        "outputId": "7ba26c43-6936-4f9a-ddd1-35fe0ea4b44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 6s/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5s/step\n",
            "Epoch 1/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.2965 - loss: 2.1337 - val_accuracy: 0.3913 - val_loss: 2.2712\n",
            "Epoch 2/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5157 - loss: 1.4309 - val_accuracy: 0.4348 - val_loss: 1.5472\n",
            "Epoch 3/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5526 - loss: 1.1597 - val_accuracy: 0.4058 - val_loss: 1.3864\n",
            "Epoch 4/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5867 - loss: 1.0281 - val_accuracy: 0.4493 - val_loss: 1.1216\n",
            "Epoch 5/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5748 - loss: 1.1331 - val_accuracy: 0.5362 - val_loss: 0.9984\n",
            "Epoch 6/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6044 - loss: 0.9768 - val_accuracy: 0.5217 - val_loss: 1.0019\n",
            "Epoch 7/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5950 - loss: 0.9926 - val_accuracy: 0.5072 - val_loss: 1.0348\n",
            "Epoch 8/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6669 - loss: 0.8408 - val_accuracy: 0.5652 - val_loss: 1.0559\n",
            "Epoch 9/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6679 - loss: 0.8332 - val_accuracy: 0.5652 - val_loss: 1.0226\n",
            "Epoch 10/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7351 - loss: 0.7081 - val_accuracy: 0.6087 - val_loss: 1.0430\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Classification Report (CNN + MLP + Fine-tuning):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  exc_solder       0.59      0.52      0.55        33\n",
            "        good       0.57      0.26      0.36        65\n",
            "     no_good       0.53      0.89      0.67        47\n",
            " poor_solder       0.25      0.42      0.31        12\n",
            "       spike       0.50      0.47      0.48        15\n",
            "\n",
            "    accuracy                           0.51       172\n",
            "   macro avg       0.49      0.51      0.47       172\n",
            "weighted avg       0.53      0.51      0.49       172\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import albumentations as A\n",
        "\n",
        "IMG_SIZE = 224\n",
        "DATA_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Labeled'\n",
        "\n",
        "# --- Augmentation ---\n",
        "augment = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=15, p=0.3),\n",
        "])\n",
        "\n",
        "# --- Load and augment images ---\n",
        "def load_images(data_folder):\n",
        "    X, y = [], []\n",
        "    for file in os.listdir(data_folder):\n",
        "        if file.endswith('.json'):\n",
        "            with open(os.path.join(data_folder, file)) as f:\n",
        "                label_data = json.load(f)\n",
        "            label = label_data['shapes'][0]['label']\n",
        "            image_path = os.path.join(data_folder, label_data['imagePath'])\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "                img = cv2.imread(image_path)\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Original\n",
        "                X.append(preprocess_input(img))\n",
        "                y.append(label)\n",
        "\n",
        "                # Augmented\n",
        "                img_aug = augment(image=img)['image']\n",
        "                X.append(preprocess_input(img_aug))\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# data loading\n",
        "X, y = load_images(DATA_FOLDER)\n",
        "\n",
        "# label encoding\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y_cat, random_state=42)\n",
        "\n",
        "# Resnet base model\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')\n",
        "\n",
        "# Freeze all layers except last 30\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Extract features\n",
        "X_train_feat = base_model.predict(X_train)\n",
        "X_test_feat = base_model.predict(X_test)\n",
        "\n",
        "# MLP classifier with batch norm, dropout\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train_feat.shape[1],)),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_cat.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    X_train_feat, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_probs = model.predict(X_test_feat)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report (CNN + MLP + Fine-tuning):\")\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Configuration ---\n",
        "SOURCE_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Dataset'  # Root folder containing defect folders\n",
        "DEST_FOLDER = '/content/drive/MyDrive/Colab Notebooks/All_Images'  # Destination folder for all images\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(DEST_FOLDER, exist_ok=True)\n",
        "\n",
        "# Traverse all directories and subdirectories\n",
        "for root, dirs, files in os.walk(SOURCE_FOLDER):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Image file check\n",
        "            source_path = os.path.join(root, file)\n",
        "            dest_path = os.path.join(DEST_FOLDER, file)\n",
        "\n",
        "            # If there's a filename conflict, rename the file\n",
        "            base, ext = os.path.splitext(file)\n",
        "            counter = 1\n",
        "            while os.path.exists(dest_path):\n",
        "                dest_path = os.path.join(DEST_FOLDER, f\"{base}_{counter}{ext}\")\n",
        "                counter += 1\n",
        "\n",
        "            shutil.copy(source_path, dest_path)\n",
        "\n",
        "print(f\"✅ All images from nested folders have been moved to: {DEST_FOLDER}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTIKZUia6ZJb",
        "outputId": "18ceb66a-c0f9-487f-e1b1-0ff9f5e4c346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All images from nested folders have been moved to: /content/drive/MyDrive/Colab Notebooks/All_Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.feature import hog\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import albumentations as A\n",
        "\n",
        "LABELED_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Labeled'\n",
        "UNLABELED_FOLDER = '/content/drive/MyDrive/Colab Notebooks/All_Images'\n",
        "IMG_SIZE = 128\n",
        "\n",
        "# Data Augmentation\n",
        "augment = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=15, p=0.3),\n",
        "])\n",
        "\n",
        "# HOG feature extractor\n",
        "def preprocess_with_hog(img):\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    features = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
        "    return features\n",
        "\n",
        "# loading labbeled dataset\n",
        "def load_labeled_data(data_folder):\n",
        "    X, y = [], []\n",
        "    for file in os.listdir(data_folder):\n",
        "        if file.endswith('.json'):\n",
        "            with open(os.path.join(data_folder, file)) as f:\n",
        "                label_data = json.load(f)\n",
        "            label = label_data['shapes'][0]['label']\n",
        "            image_path = os.path.join(data_folder, label_data['imagePath'])\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "                img = cv2.imread(image_path)\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "                # Original image\n",
        "                X.append(preprocess_with_hog(img))\n",
        "                y.append(label)\n",
        "\n",
        "                # Augmented\n",
        "                img_aug = augment(image=img)['image']\n",
        "                X.append(preprocess_with_hog(img_aug))\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Generate synthetic non-defective samples from folder\n",
        "def generate_non_defective_samples_from_folder(folder):\n",
        "    features = []\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            path = os.path.join(folder, file)\n",
        "            img = cv2.imread(path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                img = cv2.GaussianBlur(img, (5, 5), 0)  # simulate non-defect\n",
        "                features.append(preprocess_with_hog(img))\n",
        "    return np.array(features)\n",
        "\n",
        "# Load labeled data\n",
        "X, y = load_labeled_data(LABELED_FOLDER)\n",
        "\n",
        "# Generate synthetic non-defective images from defective images\n",
        "X_non_defect = generate_non_defective_samples_from_folder(UNLABELED_FOLDER)\n",
        "\n",
        "# Combine defective and synthetic non-defective samples\n",
        "X_train = np.concatenate([X, X_non_defect])\n",
        "y_train = np.concatenate([y, ['non_defective'] * len(X_non_defect)])\n",
        "\n",
        "# Split + SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train initial RF model\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "rf.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"=== Final Evaluation on Test Set ===\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3932CHonvGN",
        "outputId": "a4ae5940-de8b-4d5e-9dc8-8dab382e85cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Final Evaluation on Test Set ===\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   exc_solder       0.63      0.53      0.58        32\n",
            "         good       0.79      0.58      0.67        65\n",
            "      no_good       0.76      0.53      0.62        47\n",
            "non_defective       0.86      1.00      0.93       328\n",
            "  poor_solder       1.00      0.25      0.40        12\n",
            "        spike       0.67      0.40      0.50        15\n",
            "\n",
            "     accuracy                           0.83       499\n",
            "    macro avg       0.78      0.55      0.62       499\n",
            " weighted avg       0.83      0.83      0.82       499\n",
            "\n"
          ]
        }
      ]
    }
  ]
}