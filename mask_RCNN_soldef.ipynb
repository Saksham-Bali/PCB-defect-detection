{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.7\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzET2MCTaoRE",
        "outputId": "aaa52505-103d-4446-cb11-c62d77eaac80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.7 is already the newest version (3.7.17-1+jammy1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n"
      ],
      "metadata": {
        "id": "udlJ_cOYa04i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --config python3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYGQK9_fjYyo",
        "outputId": "cec94222-9fe3-4a91-e6fb-4c980bd2be03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.11   2         auto mode\n",
            "  1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.11   2         manual mode\n",
            "  3            /usr/bin/python3.7    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 3\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://bootstrap.pypa.io/pip/3.7/get-pip.py -o get-pip.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxa72GpojhM9",
        "outputId": "887143ca-c894-4a17-e9d5-2e4c78ac97a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2574k  100 2574k    0     0  9670k      0 --:--:-- --:--:-- --:--:-- 9641k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 get-pip.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi_3oe17jkxU",
        "outputId": "14013eaa-0fa2-4694-ea76-47b65e22d6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip<24.1\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-24.0 setuptools-68.0.0 wheel-0.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcVxUWeQjqLn",
        "outputId": "2410a32c-bafa-4b9b-b26c-7e6b233191f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install torch==1.8.0 torchvision==0.9.0\n",
        "!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppf2Q-zbkS2N",
        "outputId": "6bceadd0-5ac1-4fbd-9f7c-e71c390d4d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.0 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-ivf73szb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-ivf73szb\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 536dc9d527074e3b15df5f6677ffe1f4e104a4ab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.8)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6438260 sha256=8f72c4e216fbba0afae040d3544fa89184987514aed1e9356bef21ff22a5d58f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dyc2jjcn/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=7007fea7356509ca668ad300b6f85da47e7ae907cdc6cbc51bc96fd2aab1e40c\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import torch\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "setup_logger()\n",
        "\n",
        "# === STEP 1: Dataset Preparation ===\n",
        "\n",
        "base_folder = \"/content/drive/MyDrive/Colab Notebooks/Labeled\"\n",
        "output_base_dataset_1 = \"/content/drive/MyDrive/output_dataset_1\"\n",
        "output_base_dataset_2 = \"/content/drive/MyDrive/output_dataset_2\"\n",
        "\n",
        "class_mapping_dataset_1 = {\"good\": 0, \"no_good\": 1}\n",
        "class_mapping_dataset_2 = {\"good\": 0, \"exc_solder\": 1, \"poor_solder\": 2, \"spike\": 3}\n",
        "\n",
        "for base in [output_base_dataset_1, output_base_dataset_2]:\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(base, \"images\", split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base, \"labels\", split), exist_ok=True)\n",
        "\n",
        "train_ratio = 0.8\n",
        "json_files = [f for f in os.listdir(base_folder) if f.endswith(\".json\")]\n",
        "train_files = random.sample(json_files, int(len(json_files) * train_ratio))\n",
        "val_files = [f for f in json_files if f not in train_files]\n",
        "\n",
        "def is_valid_for_dataset(json_path, class_mapping):\n",
        "    with open(json_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    for shape in data['shapes']:\n",
        "        if shape['label'] not in class_mapping:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "for file_set, split in [(train_files, \"train\"), (val_files, \"val\")]:\n",
        "    for filename in file_set:\n",
        "        json_path = os.path.join(base_folder, filename)\n",
        "        image_name = os.path.splitext(filename)[0] + \".jpg\"\n",
        "        image_path = os.path.join(base_folder, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        # Dataset 1\n",
        "        if is_valid_for_dataset(json_path, class_mapping_dataset_1):\n",
        "            dest_img = os.path.join(output_base_dataset_1, \"images\", split, image_name)\n",
        "            dest_json = os.path.join(output_base_dataset_1, \"labels\", split, filename)\n",
        "            shutil.copy(image_path, dest_img)\n",
        "            shutil.copy(json_path, dest_json)\n",
        "\n",
        "        # Dataset 2\n",
        "        if is_valid_for_dataset(json_path, class_mapping_dataset_2):\n",
        "            dest_img = os.path.join(output_base_dataset_2, \"images\", split, image_name)\n",
        "            dest_json = os.path.join(output_base_dataset_2, \"labels\", split, filename)\n",
        "            shutil.copy(image_path, dest_img)\n",
        "            shutil.copy(json_path, dest_json)\n",
        "\n",
        "# === STEP 2: Dataset Registration for Detectron2 ===\n",
        "\n",
        "def load_soldef_detectron2(img_dir, label_dir, class_names):\n",
        "    dataset_dicts = []\n",
        "    img_files = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
        "\n",
        "    for idx, img_file in enumerate(img_files):\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        json_name = os.path.splitext(img_file)[0] + \".json\"\n",
        "        json_path = os.path.join(label_dir, json_name)\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "            continue\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        record = {\n",
        "            \"file_name\": img_path,\n",
        "            \"image_id\": idx,\n",
        "            \"height\": data.get(\"imageHeight\"),\n",
        "            \"width\": data.get(\"imageWidth\"),\n",
        "            \"annotations\": []\n",
        "        }\n",
        "\n",
        "        for shape in data.get(\"shapes\", []):\n",
        "            label = shape[\"label\"]\n",
        "            if label not in class_names:\n",
        "                continue\n",
        "            pts = shape[\"points\"]\n",
        "            poly = [float(p) for x, y in pts for p in (x, y)]\n",
        "            xs = [p[0] for p in pts]\n",
        "            ys = [p[1] for p in pts]\n",
        "            bbox = [min(xs), min(ys), max(xs), max(ys)]\n",
        "\n",
        "            record[\"annotations\"].append({\n",
        "                \"bbox\": bbox,\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": class_names.index(label)\n",
        "            })\n",
        "\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "def register_soldef_dataset(dataset_name, base_path, class_names):\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        img_dir = os.path.join(base_path, \"images\", split)\n",
        "        label_dir = os.path.join(base_path, \"labels\", split)\n",
        "        name = f\"{dataset_name}_{split}\"\n",
        "        DatasetCatalog.register(name, lambda img=img_dir, lab=label_dir, cls=class_names:\n",
        "                                load_soldef_detectron2(img, lab, cls))\n",
        "        MetadataCatalog.get(name).set(thing_classes=class_names)\n",
        "\n",
        "register_soldef_dataset(\"soldef1\", output_base_dataset_1, [\"good\", \"no_good\"])\n",
        "register_soldef_dataset(\"soldef2\", output_base_dataset_2, [\"good\", \"exc_solder\", \"poor_solder\", \"spike\"])\n",
        "\n",
        "# === STEP 3: Training + Evaluation ===\n",
        "\n",
        "def train_detectron2(dataset_name, num_classes, output_dir, max_iter=1500):\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.DATASETS.TRAIN = (f\"{dataset_name}_train\",)\n",
        "    cfg.DATASETS.TEST = (f\"{dataset_name}_val\",)\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER = max_iter\n",
        "    cfg.SOLVER.STEPS = []\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "    cfg.OUTPUT_DIR = output_dir\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "\n",
        "    # === mAP Evaluation ===\n",
        "    evaluator = COCOEvaluator(f\"{dataset_name}_val\", cfg, False, output_dir=output_dir)\n",
        "    val_loader = build_detection_test_loader(cfg, f\"{dataset_name}_val\")\n",
        "    metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "\n",
        "    print(f\"\\n========== mAP Evaluation for {dataset_name} ==========\")\n",
        "    print(metrics)\n",
        "\n",
        "# Train + evaluate both datasets\n",
        "train_detectron2(\"soldef1\", num_classes=2,\n",
        "                 output_dir=\"/content/drive/MyDrive/output_dataset_1\")\n",
        "\n",
        "train_detectron2(\"soldef2\", num_classes=4,\n",
        "                 output_dir=\"/content/drive/MyDrive/output_dataset_2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxZaocK4ljE2",
        "outputId": "ad724bb4-c905-4baf-f206-beac4944b7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/12 18:04:56 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[05/12 18:05:20 d2.data.build]: Removed 0 images with no usable annotations. 219 images left.\n",
            "[05/12 18:05:20 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|    good    | 111          |  no_good   | 113          |\n",
            "|            |              |            |              |\n",
            "|   total    | 224          |            |              |\n",
            "[05/12 18:05:20 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/12 18:05:20 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/12 18:05:20 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:05:20 d2.data.common]: Serializing 219 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:05:20 d2.data.common]: Serialized dataset takes 0.15 MiB\n",
            "[05/12 18:05:20 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[05/12 18:05:20 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:01, 121MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/12 18:05:22 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/12 18:05:38 d2.utils.events]:  eta: 0:13:25  iter: 19  total_loss: 2.142  loss_cls: 1.004  loss_box_reg: 0.4561  loss_mask: 0.6882  loss_rpn_cls: 0.005573  loss_rpn_loc: 0.007049    time: 0.5421  last_time: 0.6243  data_time: 0.1042  last_data_time: 0.0497   lr: 4.9953e-06  max_mem: 2579M\n",
            "[05/12 18:05:53 d2.utils.events]:  eta: 0:13:14  iter: 39  total_loss: 2.091  loss_cls: 0.9122  loss_box_reg: 0.4932  loss_mask: 0.6803  loss_rpn_cls: 0.005267  loss_rpn_loc: 0.006741    time: 0.5467  last_time: 0.5124  data_time: 0.0090  last_data_time: 0.0101   lr: 9.9902e-06  max_mem: 2579M\n",
            "[05/12 18:06:03 d2.utils.events]:  eta: 0:12:40  iter: 59  total_loss: 1.911  loss_cls: 0.737  loss_box_reg: 0.4834  loss_mask: 0.6634  loss_rpn_cls: 0.003947  loss_rpn_loc: 0.007088    time: 0.5357  last_time: 0.5037  data_time: 0.0170  last_data_time: 0.0082   lr: 1.4985e-05  max_mem: 2579M\n",
            "[05/12 18:06:14 d2.utils.events]:  eta: 0:12:31  iter: 79  total_loss: 1.673  loss_cls: 0.5752  loss_box_reg: 0.4875  loss_mask: 0.6385  loss_rpn_cls: 0.004591  loss_rpn_loc: 0.007243    time: 0.5415  last_time: 0.5273  data_time: 0.0253  last_data_time: 0.0058   lr: 1.998e-05  max_mem: 2579M\n",
            "[05/12 18:06:25 d2.utils.events]:  eta: 0:12:29  iter: 99  total_loss: 1.528  loss_cls: 0.4643  loss_box_reg: 0.481  loss_mask: 0.6063  loss_rpn_cls: 0.004531  loss_rpn_loc: 0.00923    time: 0.5439  last_time: 0.4889  data_time: 0.0240  last_data_time: 0.0060   lr: 2.4975e-05  max_mem: 2579M\n",
            "[05/12 18:06:36 d2.utils.events]:  eta: 0:12:31  iter: 119  total_loss: 1.47  loss_cls: 0.4089  loss_box_reg: 0.4679  loss_mask: 0.558  loss_rpn_cls: 0.003766  loss_rpn_loc: 0.007203    time: 0.5467  last_time: 0.5789  data_time: 0.0102  last_data_time: 0.0158   lr: 2.997e-05  max_mem: 2579M\n",
            "[05/12 18:06:47 d2.utils.events]:  eta: 0:12:21  iter: 139  total_loss: 1.38  loss_cls: 0.3612  loss_box_reg: 0.4805  loss_mask: 0.513  loss_rpn_cls: 0.00434  loss_rpn_loc: 0.006146    time: 0.5462  last_time: 0.6338  data_time: 0.0067  last_data_time: 0.0139   lr: 3.4965e-05  max_mem: 2579M\n",
            "[05/12 18:06:58 d2.utils.events]:  eta: 0:12:10  iter: 159  total_loss: 1.317  loss_cls: 0.3351  loss_box_reg: 0.473  loss_mask: 0.4762  loss_rpn_cls: 0.004742  loss_rpn_loc: 0.005379    time: 0.5461  last_time: 0.4851  data_time: 0.0074  last_data_time: 0.0058   lr: 3.996e-05  max_mem: 2579M\n",
            "[05/12 18:07:10 d2.utils.events]:  eta: 0:12:01  iter: 179  total_loss: 1.283  loss_cls: 0.3165  loss_box_reg: 0.4886  loss_mask: 0.4376  loss_rpn_cls: 0.001533  loss_rpn_loc: 0.007951    time: 0.5484  last_time: 0.5297  data_time: 0.0099  last_data_time: 0.0062   lr: 4.4955e-05  max_mem: 2581M\n",
            "[05/12 18:07:21 d2.utils.events]:  eta: 0:11:52  iter: 199  total_loss: 1.212  loss_cls: 0.2892  loss_box_reg: 0.4864  loss_mask: 0.394  loss_rpn_cls: 0.0009179  loss_rpn_loc: 0.004258    time: 0.5487  last_time: 0.5040  data_time: 0.0149  last_data_time: 0.0048   lr: 4.995e-05  max_mem: 2581M\n",
            "[05/12 18:07:32 d2.utils.events]:  eta: 0:11:41  iter: 219  total_loss: 1.14  loss_cls: 0.2631  loss_box_reg: 0.4989  loss_mask: 0.367  loss_rpn_cls: 0.001368  loss_rpn_loc: 0.004815    time: 0.5499  last_time: 0.7009  data_time: 0.0157  last_data_time: 0.0652   lr: 5.4945e-05  max_mem: 2581M\n",
            "[05/12 18:07:43 d2.utils.events]:  eta: 0:11:31  iter: 239  total_loss: 1.127  loss_cls: 0.2561  loss_box_reg: 0.5187  loss_mask: 0.3351  loss_rpn_cls: 0.0003485  loss_rpn_loc: 0.005098    time: 0.5501  last_time: 0.5861  data_time: 0.0070  last_data_time: 0.0056   lr: 5.994e-05  max_mem: 2581M\n",
            "[05/12 18:07:54 d2.utils.events]:  eta: 0:11:21  iter: 259  total_loss: 1.035  loss_cls: 0.2311  loss_box_reg: 0.5125  loss_mask: 0.3004  loss_rpn_cls: 0.0004737  loss_rpn_loc: 0.004752    time: 0.5512  last_time: 0.4601  data_time: 0.0102  last_data_time: 0.0058   lr: 6.4935e-05  max_mem: 2581M\n",
            "[05/12 18:08:06 d2.utils.events]:  eta: 0:11:15  iter: 279  total_loss: 1.036  loss_cls: 0.2193  loss_box_reg: 0.5449  loss_mask: 0.2532  loss_rpn_cls: 0.0001531  loss_rpn_loc: 0.004891    time: 0.5545  last_time: 0.4673  data_time: 0.0154  last_data_time: 0.0053   lr: 6.993e-05  max_mem: 2581M\n",
            "[05/12 18:08:18 d2.utils.events]:  eta: 0:11:06  iter: 299  total_loss: 1.028  loss_cls: 0.216  loss_box_reg: 0.5587  loss_mask: 0.2025  loss_rpn_cls: 8.426e-05  loss_rpn_loc: 0.005023    time: 0.5564  last_time: 0.5683  data_time: 0.0089  last_data_time: 0.0062   lr: 7.4925e-05  max_mem: 2581M\n",
            "[05/12 18:08:29 d2.utils.events]:  eta: 0:10:57  iter: 319  total_loss: 0.9107  loss_cls: 0.2052  loss_box_reg: 0.5336  loss_mask: 0.1669  loss_rpn_cls: 6.643e-05  loss_rpn_loc: 0.004327    time: 0.5574  last_time: 0.6698  data_time: 0.0069  last_data_time: 0.0132   lr: 7.992e-05  max_mem: 2581M\n",
            "[05/12 18:08:40 d2.utils.events]:  eta: 0:10:44  iter: 339  total_loss: 0.8766  loss_cls: 0.2058  loss_box_reg: 0.5182  loss_mask: 0.1514  loss_rpn_cls: 1.831e-05  loss_rpn_loc: 0.004295    time: 0.5567  last_time: 0.5867  data_time: 0.0066  last_data_time: 0.0054   lr: 8.4915e-05  max_mem: 2581M\n",
            "[05/12 18:08:52 d2.utils.events]:  eta: 0:10:35  iter: 359  total_loss: 0.8253  loss_cls: 0.2224  loss_box_reg: 0.4819  loss_mask: 0.1058  loss_rpn_cls: 9.041e-06  loss_rpn_loc: 0.006225    time: 0.5582  last_time: 0.5178  data_time: 0.0088  last_data_time: 0.0058   lr: 8.991e-05  max_mem: 2581M\n",
            "[05/12 18:09:03 d2.utils.events]:  eta: 0:10:25  iter: 379  total_loss: 0.6354  loss_cls: 0.2071  loss_box_reg: 0.318  loss_mask: 0.1004  loss_rpn_cls: 1.578e-06  loss_rpn_loc: 0.005256    time: 0.5589  last_time: 0.4650  data_time: 0.0090  last_data_time: 0.0055   lr: 9.4905e-05  max_mem: 2581M\n",
            "[05/12 18:09:15 d2.utils.events]:  eta: 0:10:16  iter: 399  total_loss: 0.6147  loss_cls: 0.2435  loss_box_reg: 0.274  loss_mask: 0.08867  loss_rpn_cls: 4.117e-06  loss_rpn_loc: 0.006132    time: 0.5606  last_time: 0.6087  data_time: 0.0106  last_data_time: 0.0078   lr: 9.99e-05  max_mem: 2581M\n",
            "[05/12 18:09:27 d2.utils.events]:  eta: 0:10:06  iter: 419  total_loss: 0.5529  loss_cls: 0.2264  loss_box_reg: 0.2316  loss_mask: 0.08274  loss_rpn_cls: 6.916e-07  loss_rpn_loc: 0.004114    time: 0.5613  last_time: 0.5943  data_time: 0.0079  last_data_time: 0.0056   lr: 0.0001049  max_mem: 2581M\n",
            "[05/12 18:09:38 d2.utils.events]:  eta: 0:09:54  iter: 439  total_loss: 0.5322  loss_cls: 0.2268  loss_box_reg: 0.2277  loss_mask: 0.0774  loss_rpn_cls: 2.393e-06  loss_rpn_loc: 0.005726    time: 0.5614  last_time: 0.6258  data_time: 0.0060  last_data_time: 0.0143   lr: 0.00010989  max_mem: 2581M\n",
            "[05/12 18:09:49 d2.utils.events]:  eta: 0:09:42  iter: 459  total_loss: 0.4856  loss_cls: 0.2067  loss_box_reg: 0.1922  loss_mask: 0.07472  loss_rpn_cls: 1.318e-06  loss_rpn_loc: 0.004876    time: 0.5612  last_time: 0.5915  data_time: 0.0075  last_data_time: 0.0070   lr: 0.00011489  max_mem: 2581M\n",
            "[05/12 18:10:01 d2.utils.events]:  eta: 0:09:32  iter: 479  total_loss: 0.4784  loss_cls: 0.2109  loss_box_reg: 0.1836  loss_mask: 0.06708  loss_rpn_cls: 1.172e-05  loss_rpn_loc: 0.005333    time: 0.5622  last_time: 0.4658  data_time: 0.0103  last_data_time: 0.0059   lr: 0.00011988  max_mem: 2581M\n",
            "[05/12 18:10:12 d2.utils.events]:  eta: 0:09:21  iter: 499  total_loss: 0.4355  loss_cls: 0.2266  loss_box_reg: 0.1534  loss_mask: 0.07101  loss_rpn_cls: 7.337e-07  loss_rpn_loc: 0.004024    time: 0.5628  last_time: 0.5195  data_time: 0.0082  last_data_time: 0.0060   lr: 0.00012488  max_mem: 2581M\n",
            "[05/12 18:10:24 d2.utils.events]:  eta: 0:09:10  iter: 519  total_loss: 0.3415  loss_cls: 0.156  loss_box_reg: 0.1265  loss_mask: 0.06912  loss_rpn_cls: 1.621e-06  loss_rpn_loc: 0.00332    time: 0.5636  last_time: 0.5359  data_time: 0.0085  last_data_time: 0.0054   lr: 0.00012987  max_mem: 2581M\n",
            "[05/12 18:10:36 d2.utils.events]:  eta: 0:09:00  iter: 539  total_loss: 0.3976  loss_cls: 0.1691  loss_box_reg: 0.1453  loss_mask: 0.06716  loss_rpn_cls: 1.094e-06  loss_rpn_loc: 0.003506    time: 0.5643  last_time: 0.6302  data_time: 0.0090  last_data_time: 0.0200   lr: 0.00013487  max_mem: 2581M\n",
            "[05/12 18:10:47 d2.utils.events]:  eta: 0:08:49  iter: 559  total_loss: 0.3626  loss_cls: 0.1789  loss_box_reg: 0.1269  loss_mask: 0.06488  loss_rpn_cls: 2.084e-06  loss_rpn_loc: 0.002964    time: 0.5649  last_time: 0.6168  data_time: 0.0070  last_data_time: 0.0096   lr: 0.00013986  max_mem: 2581M\n",
            "[05/12 18:10:59 d2.utils.events]:  eta: 0:08:39  iter: 579  total_loss: 0.3895  loss_cls: 0.1815  loss_box_reg: 0.1187  loss_mask: 0.06736  loss_rpn_cls: 2.636e-06  loss_rpn_loc: 0.002864    time: 0.5655  last_time: 0.5829  data_time: 0.0114  last_data_time: 0.0128   lr: 0.00014486  max_mem: 2581M\n",
            "[05/12 18:11:11 d2.utils.events]:  eta: 0:08:29  iter: 599  total_loss: 0.3372  loss_cls: 0.1714  loss_box_reg: 0.1135  loss_mask: 0.06211  loss_rpn_cls: 1.796e-06  loss_rpn_loc: 0.004467    time: 0.5661  last_time: 0.5922  data_time: 0.0095  last_data_time: 0.0051   lr: 0.00014985  max_mem: 2581M\n",
            "[05/12 18:11:22 d2.utils.events]:  eta: 0:08:18  iter: 619  total_loss: 0.3486  loss_cls: 0.1592  loss_box_reg: 0.1163  loss_mask: 0.06331  loss_rpn_cls: 2.223e-06  loss_rpn_loc: 0.002519    time: 0.5664  last_time: 0.5309  data_time: 0.0080  last_data_time: 0.0054   lr: 0.00015485  max_mem: 2581M\n",
            "[05/12 18:11:34 d2.utils.events]:  eta: 0:08:07  iter: 639  total_loss: 0.351  loss_cls: 0.1618  loss_box_reg: 0.1226  loss_mask: 0.06136  loss_rpn_cls: 1.656e-06  loss_rpn_loc: 0.002755    time: 0.5671  last_time: 0.5757  data_time: 0.0099  last_data_time: 0.0053   lr: 0.00015984  max_mem: 2581M\n",
            "[05/12 18:11:45 d2.utils.events]:  eta: 0:07:56  iter: 659  total_loss: 0.3871  loss_cls: 0.1697  loss_box_reg: 0.1314  loss_mask: 0.06243  loss_rpn_cls: 3.228e-06  loss_rpn_loc: 0.002715    time: 0.5672  last_time: 0.6452  data_time: 0.0061  last_data_time: 0.0107   lr: 0.00016484  max_mem: 2581M\n",
            "[05/12 18:11:57 d2.utils.events]:  eta: 0:07:45  iter: 679  total_loss: 0.3303  loss_cls: 0.1646  loss_box_reg: 0.1073  loss_mask: 0.05592  loss_rpn_cls: 2.544e-06  loss_rpn_loc: 0.002775    time: 0.5671  last_time: 0.5958  data_time: 0.0068  last_data_time: 0.0064   lr: 0.00016983  max_mem: 2581M\n",
            "[05/12 18:12:08 d2.utils.events]:  eta: 0:07:34  iter: 699  total_loss: 0.3287  loss_cls: 0.1608  loss_box_reg: 0.1138  loss_mask: 0.05727  loss_rpn_cls: 1.107e-06  loss_rpn_loc: 0.002766    time: 0.5675  last_time: 0.4941  data_time: 0.0117  last_data_time: 0.0053   lr: 0.00017483  max_mem: 2581M\n",
            "[05/12 18:12:20 d2.utils.events]:  eta: 0:07:22  iter: 719  total_loss: 0.2975  loss_cls: 0.1605  loss_box_reg: 0.08787  loss_mask: 0.05354  loss_rpn_cls: 1.764e-06  loss_rpn_loc: 0.002432    time: 0.5678  last_time: 0.5889  data_time: 0.0082  last_data_time: 0.0057   lr: 0.00017982  max_mem: 2581M\n",
            "[05/12 18:12:32 d2.utils.events]:  eta: 0:07:11  iter: 739  total_loss: 0.3196  loss_cls: 0.1607  loss_box_reg: 0.1033  loss_mask: 0.05898  loss_rpn_cls: 8.713e-07  loss_rpn_loc: 0.00241    time: 0.5682  last_time: 0.6229  data_time: 0.0069  last_data_time: 0.0053   lr: 0.00018482  max_mem: 2581M\n",
            "[05/12 18:12:44 d2.utils.events]:  eta: 0:07:01  iter: 759  total_loss: 0.3041  loss_cls: 0.152  loss_box_reg: 0.09244  loss_mask: 0.05107  loss_rpn_cls: 1.005e-06  loss_rpn_loc: 0.002454    time: 0.5693  last_time: 0.5646  data_time: 0.0149  last_data_time: 0.0173   lr: 0.00018981  max_mem: 2581M\n",
            "[05/12 18:12:55 d2.utils.events]:  eta: 0:06:49  iter: 779  total_loss: 0.3114  loss_cls: 0.1517  loss_box_reg: 0.1006  loss_mask: 0.06161  loss_rpn_cls: 4.102e-07  loss_rpn_loc: 0.002509    time: 0.5691  last_time: 0.6093  data_time: 0.0062  last_data_time: 0.0084   lr: 0.00019481  max_mem: 2581M\n",
            "[05/12 18:13:06 d2.utils.events]:  eta: 0:06:38  iter: 799  total_loss: 0.3248  loss_cls: 0.1435  loss_box_reg: 0.1144  loss_mask: 0.05565  loss_rpn_cls: 1.652e-06  loss_rpn_loc: 0.002025    time: 0.5690  last_time: 0.5732  data_time: 0.0094  last_data_time: 0.0054   lr: 0.0001998  max_mem: 2581M\n",
            "[05/12 18:13:18 d2.utils.events]:  eta: 0:06:27  iter: 819  total_loss: 0.3097  loss_cls: 0.1555  loss_box_reg: 0.09741  loss_mask: 0.05159  loss_rpn_cls: 2.226e-06  loss_rpn_loc: 0.00226    time: 0.5692  last_time: 0.5484  data_time: 0.0082  last_data_time: 0.0052   lr: 0.0002048  max_mem: 2581M\n",
            "[05/12 18:13:30 d2.utils.events]:  eta: 0:06:15  iter: 839  total_loss: 0.2918  loss_cls: 0.1415  loss_box_reg: 0.092  loss_mask: 0.0505  loss_rpn_cls: 4.735e-07  loss_rpn_loc: 0.002319    time: 0.5695  last_time: 0.5942  data_time: 0.0098  last_data_time: 0.0058   lr: 0.00020979  max_mem: 2581M\n",
            "[05/12 18:13:41 d2.utils.events]:  eta: 0:06:04  iter: 859  total_loss: 0.2814  loss_cls: 0.1376  loss_box_reg: 0.08576  loss_mask: 0.04996  loss_rpn_cls: 1.156e-06  loss_rpn_loc: 0.002174    time: 0.5700  last_time: 0.5931  data_time: 0.0105  last_data_time: 0.0052   lr: 0.00021479  max_mem: 2581M\n",
            "[05/12 18:13:53 d2.utils.events]:  eta: 0:05:53  iter: 879  total_loss: 0.2826  loss_cls: 0.1274  loss_box_reg: 0.09825  loss_mask: 0.05002  loss_rpn_cls: 1.813e-06  loss_rpn_loc: 0.002129    time: 0.5700  last_time: 0.6390  data_time: 0.0085  last_data_time: 0.0229   lr: 0.00021978  max_mem: 2581M\n",
            "[05/12 18:14:04 d2.utils.events]:  eta: 0:05:42  iter: 899  total_loss: 0.2484  loss_cls: 0.1008  loss_box_reg: 0.08339  loss_mask: 0.04975  loss_rpn_cls: 3.505e-06  loss_rpn_loc: 0.00238    time: 0.5697  last_time: 0.6214  data_time: 0.0072  last_data_time: 0.0056   lr: 0.00022478  max_mem: 2581M\n",
            "[05/12 18:14:16 d2.utils.events]:  eta: 0:05:31  iter: 919  total_loss: 0.2526  loss_cls: 0.1056  loss_box_reg: 0.09023  loss_mask: 0.04955  loss_rpn_cls: 1.837e-06  loss_rpn_loc: 0.002693    time: 0.5701  last_time: 0.6007  data_time: 0.0109  last_data_time: 0.0050   lr: 0.00022977  max_mem: 2581M\n",
            "[05/12 18:14:27 d2.utils.events]:  eta: 0:05:19  iter: 939  total_loss: 0.2843  loss_cls: 0.1165  loss_box_reg: 0.09712  loss_mask: 0.05512  loss_rpn_cls: 1.96e-06  loss_rpn_loc: 0.001952    time: 0.5703  last_time: 0.5494  data_time: 0.0101  last_data_time: 0.0052   lr: 0.00023477  max_mem: 2581M\n",
            "[05/12 18:14:39 d2.utils.events]:  eta: 0:05:08  iter: 959  total_loss: 0.246  loss_cls: 0.09537  loss_box_reg: 0.09449  loss_mask: 0.05355  loss_rpn_cls: 1.765e-06  loss_rpn_loc: 0.002669    time: 0.5707  last_time: 0.5906  data_time: 0.0093  last_data_time: 0.0058   lr: 0.00023976  max_mem: 2581M\n",
            "[05/12 18:14:50 d2.utils.events]:  eta: 0:04:57  iter: 979  total_loss: 0.3175  loss_cls: 0.1458  loss_box_reg: 0.09216  loss_mask: 0.05309  loss_rpn_cls: 1.691e-06  loss_rpn_loc: 0.002305    time: 0.5706  last_time: 0.4983  data_time: 0.0094  last_data_time: 0.0054   lr: 0.00024476  max_mem: 2581M\n",
            "[05/12 18:15:02 d2.utils.events]:  eta: 0:04:45  iter: 999  total_loss: 0.2666  loss_cls: 0.1269  loss_box_reg: 0.09872  loss_mask: 0.05684  loss_rpn_cls: 2.31e-06  loss_rpn_loc: 0.001729    time: 0.5705  last_time: 0.6516  data_time: 0.0088  last_data_time: 0.0174   lr: 0.00024975  max_mem: 2581M\n",
            "[05/12 18:15:13 d2.utils.events]:  eta: 0:04:34  iter: 1019  total_loss: 0.2725  loss_cls: 0.1244  loss_box_reg: 0.08814  loss_mask: 0.048  loss_rpn_cls: 5.05e-07  loss_rpn_loc: 0.002045    time: 0.5703  last_time: 0.4284  data_time: 0.0081  last_data_time: 0.0064   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:15:24 d2.utils.events]:  eta: 0:04:23  iter: 1039  total_loss: 0.2634  loss_cls: 0.1095  loss_box_reg: 0.09429  loss_mask: 0.04748  loss_rpn_cls: 4.873e-07  loss_rpn_loc: 0.002476    time: 0.5703  last_time: 0.5310  data_time: 0.0089  last_data_time: 0.0057   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:15:36 d2.utils.events]:  eta: 0:04:12  iter: 1059  total_loss: 0.2754  loss_cls: 0.1115  loss_box_reg: 0.103  loss_mask: 0.04983  loss_rpn_cls: 3.529e-06  loss_rpn_loc: 0.00246    time: 0.5706  last_time: 0.6088  data_time: 0.0080  last_data_time: 0.0057   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:15:47 d2.utils.events]:  eta: 0:04:00  iter: 1079  total_loss: 0.2696  loss_cls: 0.1232  loss_box_reg: 0.0928  loss_mask: 0.05427  loss_rpn_cls: 2.134e-06  loss_rpn_loc: 0.00221    time: 0.5704  last_time: 0.4832  data_time: 0.0132  last_data_time: 0.0051   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:15:59 d2.utils.events]:  eta: 0:03:49  iter: 1099  total_loss: 0.2831  loss_cls: 0.1216  loss_box_reg: 0.09018  loss_mask: 0.04871  loss_rpn_cls: 5.626e-07  loss_rpn_loc: 0.002306    time: 0.5706  last_time: 0.5895  data_time: 0.0062  last_data_time: 0.0050   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:16:10 d2.utils.events]:  eta: 0:03:38  iter: 1119  total_loss: 0.2421  loss_cls: 0.1064  loss_box_reg: 0.08391  loss_mask: 0.04709  loss_rpn_cls: 1.406e-06  loss_rpn_loc: 0.002315    time: 0.5706  last_time: 0.6649  data_time: 0.0067  last_data_time: 0.0052   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:16:22 d2.utils.events]:  eta: 0:03:27  iter: 1139  total_loss: 0.2305  loss_cls: 0.09035  loss_box_reg: 0.08749  loss_mask: 0.04852  loss_rpn_cls: 4.4e-07  loss_rpn_loc: 0.002356    time: 0.5705  last_time: 0.5922  data_time: 0.0098  last_data_time: 0.0115   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:16:33 d2.utils.events]:  eta: 0:03:15  iter: 1159  total_loss: 0.2267  loss_cls: 0.09927  loss_box_reg: 0.07639  loss_mask: 0.04744  loss_rpn_cls: 8.452e-07  loss_rpn_loc: 0.001758    time: 0.5704  last_time: 0.5857  data_time: 0.0114  last_data_time: 0.0074   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:16:44 d2.utils.events]:  eta: 0:03:04  iter: 1179  total_loss: 0.2577  loss_cls: 0.1028  loss_box_reg: 0.08763  loss_mask: 0.05007  loss_rpn_cls: 3.02e-07  loss_rpn_loc: 0.001802    time: 0.5704  last_time: 0.5303  data_time: 0.0113  last_data_time: 0.0053   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:16:56 d2.utils.events]:  eta: 0:02:52  iter: 1199  total_loss: 0.2223  loss_cls: 0.09056  loss_box_reg: 0.08219  loss_mask: 0.04546  loss_rpn_cls: 2.506e-06  loss_rpn_loc: 0.002418    time: 0.5704  last_time: 0.5780  data_time: 0.0090  last_data_time: 0.0062   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:17:07 d2.utils.events]:  eta: 0:02:41  iter: 1219  total_loss: 0.1979  loss_cls: 0.05812  loss_box_reg: 0.08252  loss_mask: 0.04665  loss_rpn_cls: 4.256e-06  loss_rpn_loc: 0.002219    time: 0.5703  last_time: 0.6470  data_time: 0.0089  last_data_time: 0.0199   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:17:19 d2.utils.events]:  eta: 0:02:29  iter: 1239  total_loss: 0.2838  loss_cls: 0.1249  loss_box_reg: 0.09519  loss_mask: 0.04733  loss_rpn_cls: 1.36e-06  loss_rpn_loc: 0.001691    time: 0.5708  last_time: 0.5959  data_time: 0.0095  last_data_time: 0.0052   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:17:31 d2.utils.events]:  eta: 0:02:18  iter: 1259  total_loss: 0.2376  loss_cls: 0.07068  loss_box_reg: 0.09242  loss_mask: 0.04732  loss_rpn_cls: 9.766e-07  loss_rpn_loc: 0.001783    time: 0.5711  last_time: 0.5924  data_time: 0.0079  last_data_time: 0.0061   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:17:43 d2.utils.events]:  eta: 0:02:06  iter: 1279  total_loss: 0.2295  loss_cls: 0.09957  loss_box_reg: 0.08774  loss_mask: 0.04513  loss_rpn_cls: 2.958e-06  loss_rpn_loc: 0.002181    time: 0.5715  last_time: 0.5642  data_time: 0.0089  last_data_time: 0.0056   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:17:54 d2.utils.events]:  eta: 0:01:55  iter: 1299  total_loss: 0.2479  loss_cls: 0.0944  loss_box_reg: 0.09779  loss_mask: 0.04684  loss_rpn_cls: 1.432e-06  loss_rpn_loc: 0.001919    time: 0.5716  last_time: 0.5720  data_time: 0.0098  last_data_time: 0.0055   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:18:06 d2.utils.events]:  eta: 0:01:43  iter: 1319  total_loss: 0.2223  loss_cls: 0.08226  loss_box_reg: 0.08861  loss_mask: 0.04328  loss_rpn_cls: 8.751e-07  loss_rpn_loc: 0.00214    time: 0.5717  last_time: 0.4961  data_time: 0.0090  last_data_time: 0.0063   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:18:17 d2.utils.events]:  eta: 0:01:32  iter: 1339  total_loss: 0.2236  loss_cls: 0.07534  loss_box_reg: 0.09302  loss_mask: 0.0447  loss_rpn_cls: 8.444e-07  loss_rpn_loc: 0.002021    time: 0.5717  last_time: 0.6548  data_time: 0.0072  last_data_time: 0.0113   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:18:29 d2.utils.events]:  eta: 0:01:20  iter: 1359  total_loss: 0.2638  loss_cls: 0.1171  loss_box_reg: 0.08642  loss_mask: 0.04415  loss_rpn_cls: 3.057e-06  loss_rpn_loc: 0.002633    time: 0.5717  last_time: 0.5624  data_time: 0.0071  last_data_time: 0.0064   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:18:41 d2.utils.events]:  eta: 0:01:09  iter: 1379  total_loss: 0.2642  loss_cls: 0.1378  loss_box_reg: 0.07647  loss_mask: 0.04632  loss_rpn_cls: 1.281e-06  loss_rpn_loc: 0.002025    time: 0.5718  last_time: 0.5376  data_time: 0.0082  last_data_time: 0.0098   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:18:52 d2.utils.events]:  eta: 0:00:57  iter: 1399  total_loss: 0.2115  loss_cls: 0.09192  loss_box_reg: 0.07281  loss_mask: 0.04481  loss_rpn_cls: 3.828e-07  loss_rpn_loc: 0.001888    time: 0.5721  last_time: 0.5835  data_time: 0.0068  last_data_time: 0.0062   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:04 d2.utils.events]:  eta: 0:00:46  iter: 1419  total_loss: 0.1865  loss_cls: 0.04908  loss_box_reg: 0.06837  loss_mask: 0.0459  loss_rpn_cls: 1.731e-06  loss_rpn_loc: 0.002082    time: 0.5721  last_time: 0.5660  data_time: 0.0096  last_data_time: 0.0085   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:15 d2.utils.events]:  eta: 0:00:34  iter: 1439  total_loss: 0.2168  loss_cls: 0.07467  loss_box_reg: 0.07909  loss_mask: 0.04601  loss_rpn_cls: 4.081e-06  loss_rpn_loc: 0.002    time: 0.5722  last_time: 0.9204  data_time: 0.0089  last_data_time: 0.0165   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:27 d2.utils.events]:  eta: 0:00:23  iter: 1459  total_loss: 0.1977  loss_cls: 0.06165  loss_box_reg: 0.07699  loss_mask: 0.04635  loss_rpn_cls: 9.722e-07  loss_rpn_loc: 0.002021    time: 0.5720  last_time: 0.6208  data_time: 0.0059  last_data_time: 0.0084   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:38 d2.utils.events]:  eta: 0:00:11  iter: 1479  total_loss: 0.2043  loss_cls: 0.07103  loss_box_reg: 0.07735  loss_mask: 0.04298  loss_rpn_cls: 5.391e-06  loss_rpn_loc: 0.001715    time: 0.5719  last_time: 0.5524  data_time: 0.0073  last_data_time: 0.0056   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:50 d2.utils.events]:  eta: 0:00:00  iter: 1499  total_loss: 0.184  loss_cls: 0.05445  loss_box_reg: 0.06652  loss_mask: 0.0451  loss_rpn_cls: 1.317e-06  loss_rpn_loc: 0.002013    time: 0.5719  last_time: 0.5565  data_time: 0.0083  last_data_time: 0.0052   lr: 0.00025  max_mem: 2581M\n",
            "[05/12 18:19:50 d2.engine.hooks]: Overall training speed: 1498 iterations in 0:14:16 (0.5719 s / it)\n",
            "[05/12 18:19:50 d2.engine.hooks]: Total training time: 0:14:22 (0:00:05 on hooks)\n",
            "[05/12 18:20:05 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|    good    | 42           |  no_good   | 42           |\n",
            "|            |              |            |              |\n",
            "|   total    | 84           |            |              |\n",
            "[05/12 18:20:05 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/12 18:20:05 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:20:05 d2.data.common]: Serializing 83 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:20:05 d2.data.common]: Serialized dataset takes 0.06 MiB\n",
            "WARNING [05/12 18:20:05 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "WARNING [05/12 18:20:05 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/12 18:20:05 d2.evaluation.coco_evaluation]: Trying to convert 'soldef1_val' to COCO format ...\n",
            "[05/12 18:20:05 d2.data.datasets.coco]: Converting annotations of dataset 'soldef1_val' to COCO format ...)\n",
            "[05/12 18:20:06 d2.data.datasets.coco]: Converting dataset dicts into COCO format\n",
            "[05/12 18:20:06 d2.data.datasets.coco]: Conversion finished, #images: 83, #annotations: 84\n",
            "[05/12 18:20:06 d2.data.datasets.coco]: Caching COCO format annotations at '/content/drive/MyDrive/output_dataset_1/soldef1_val_coco_format.json' ...\n",
            "[05/12 18:20:06 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/12 18:20:06 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:20:06 d2.data.common]: Serializing 83 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:20:06 d2.data.common]: Serialized dataset takes 0.06 MiB\n",
            "[05/12 18:20:06 d2.evaluation.evaluator]: Start inference on 83 batches\n",
            "[05/12 18:20:10 d2.evaluation.evaluator]: Inference done 11/83. Dataloading: 0.0666 s/iter. Inference: 0.1269 s/iter. Eval: 0.0457 s/iter. Total: 0.2392 s/iter. ETA=0:00:17\n",
            "[05/12 18:20:15 d2.evaluation.evaluator]: Inference done 29/83. Dataloading: 0.1132 s/iter. Inference: 0.1202 s/iter. Eval: 0.0346 s/iter. Total: 0.2682 s/iter. ETA=0:00:14\n",
            "[05/12 18:20:20 d2.evaluation.evaluator]: Inference done 48/83. Dataloading: 0.1064 s/iter. Inference: 0.1243 s/iter. Eval: 0.0406 s/iter. Total: 0.2714 s/iter. ETA=0:00:09\n",
            "[05/12 18:20:25 d2.evaluation.evaluator]: Inference done 70/83. Dataloading: 0.0874 s/iter. Inference: 0.1296 s/iter. Eval: 0.0399 s/iter. Total: 0.2572 s/iter. ETA=0:00:03\n",
            "[05/12 18:20:28 d2.evaluation.evaluator]: Total inference time: 0:00:19.009539 (0.243712 s / iter per device, on 1 devices)\n",
            "[05/12 18:20:28 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:10 (0.129313 s / iter per device, on 1 devices)\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Saving results to /content/drive/MyDrive/output_dataset_1/coco_instances_results.json\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.00 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.870\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.945\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.927\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.919\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.939\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.939\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.939\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 87.025 | 94.483 | 92.671 |  nan  |  nan  | 87.025 |\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| good       | 82.386 | no_good    | 91.664 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/12 18:20:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.893\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.945\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.945\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.893\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.946\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.967\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.967\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.967\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 89.335 | 94.483 | 94.483 |  nan  |  nan  | 89.335 |\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[05/12 18:20:28 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| good       | 86.125 | no_good    | 92.544 |\n",
            "\n",
            "========== mAP Evaluation for soldef1 ==========\n",
            "OrderedDict([('bbox', {'AP': 87.0251597193419, 'AP50': 94.48329876068622, 'AP75': 92.67067411410397, 'APs': nan, 'APm': nan, 'APl': 87.0251597193419, 'AP-good': 82.38641936846491, 'AP-no_good': 91.66390007021889}), ('segm', {'AP': 89.33476005121685, 'AP50': 94.48329876068622, 'AP75': 94.48329876068622, 'APs': nan, 'APm': nan, 'APl': 89.33476005121685, 'AP-good': 86.12526172922794, 'AP-no_good': 92.54425837320575})])\n",
            "[05/12 18:20:28 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[05/12 18:20:47 d2.data.build]: Removed 0 images with no usable annotations. 287 images left.\n",
            "[05/12 18:20:47 d2.data.build]: Distribution of instances among all 4 categories:\n",
            "|  category  | #instances   |  category  | #instances   |  category   | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:-----------:|:-------------|\n",
            "|    good    | 170          | exc_solder | 167          | poor_solder | 51           |\n",
            "|   spike    | 127          |            |              |             |              |\n",
            "|   total    | 515          |            |              |             |              |\n",
            "[05/12 18:20:47 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/12 18:20:47 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/12 18:20:47 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:20:47 d2.data.common]: Serializing 287 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:20:47 d2.data.common]: Serialized dataset takes 0.34 MiB\n",
            "[05/12 18:20:47 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[05/12 18:20:47 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (4, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/12 18:20:47 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[05/12 18:20:59 d2.utils.events]:  eta: 0:14:02  iter: 19  total_loss: 2.986  loss_cls: 1.568  loss_box_reg: 0.6722  loss_mask: 0.6899  loss_rpn_cls: 0.03454  loss_rpn_loc: 0.01597    time: 0.5729  last_time: 0.6959  data_time: 0.0645  last_data_time: 0.0894   lr: 4.9953e-06  max_mem: 2581M\n",
            "[05/12 18:21:11 d2.utils.events]:  eta: 0:14:00  iter: 39  total_loss: 2.88  loss_cls: 1.436  loss_box_reg: 0.7308  loss_mask: 0.6918  loss_rpn_cls: 0.0196  loss_rpn_loc: 0.01345    time: 0.5805  last_time: 0.5561  data_time: 0.0078  last_data_time: 0.0060   lr: 9.9902e-06  max_mem: 2581M\n",
            "[05/12 18:21:23 d2.utils.events]:  eta: 0:13:44  iter: 59  total_loss: 2.597  loss_cls: 1.195  loss_box_reg: 0.6687  loss_mask: 0.6858  loss_rpn_cls: 0.02166  loss_rpn_loc: 0.01141    time: 0.5788  last_time: 0.5733  data_time: 0.0147  last_data_time: 0.0092   lr: 1.4985e-05  max_mem: 2581M\n",
            "[05/12 18:21:34 d2.utils.events]:  eta: 0:13:28  iter: 79  total_loss: 2.192  loss_cls: 0.9308  loss_box_reg: 0.6284  loss_mask: 0.6732  loss_rpn_cls: 0.02309  loss_rpn_loc: 0.009697    time: 0.5720  last_time: 0.5564  data_time: 0.0254  last_data_time: 0.0057   lr: 1.998e-05  max_mem: 2581M\n",
            "[05/12 18:21:47 d2.utils.events]:  eta: 0:13:23  iter: 99  total_loss: 2.13  loss_cls: 0.7453  loss_box_reg: 0.6618  loss_mask: 0.6634  loss_rpn_cls: 0.02069  loss_rpn_loc: 0.01026    time: 0.5904  last_time: 0.5805  data_time: 0.1029  last_data_time: 0.0051   lr: 2.4975e-05  max_mem: 2582M\n",
            "[05/12 18:21:59 d2.utils.events]:  eta: 0:13:15  iter: 119  total_loss: 1.956  loss_cls: 0.6285  loss_box_reg: 0.6349  loss_mask: 0.6509  loss_rpn_cls: 0.01991  loss_rpn_loc: 0.01151    time: 0.5903  last_time: 0.5354  data_time: 0.0320  last_data_time: 0.0058   lr: 2.997e-05  max_mem: 2582M\n",
            "[05/12 18:22:10 d2.utils.events]:  eta: 0:12:55  iter: 139  total_loss: 1.957  loss_cls: 0.6162  loss_box_reg: 0.6612  loss_mask: 0.645  loss_rpn_cls: 0.0198  loss_rpn_loc: 0.009376    time: 0.5844  last_time: 0.6911  data_time: 0.0172  last_data_time: 0.0513   lr: 3.4965e-05  max_mem: 2582M\n",
            "[05/12 18:22:22 d2.utils.events]:  eta: 0:12:48  iter: 159  total_loss: 2.095  loss_cls: 0.6439  loss_box_reg: 0.7528  loss_mask: 0.6276  loss_rpn_cls: 0.0137  loss_rpn_loc: 0.01067    time: 0.5840  last_time: 0.6032  data_time: 0.0073  last_data_time: 0.0055   lr: 3.996e-05  max_mem: 2582M\n",
            "[05/12 18:22:33 d2.utils.events]:  eta: 0:12:37  iter: 179  total_loss: 1.897  loss_cls: 0.552  loss_box_reg: 0.6993  loss_mask: 0.6089  loss_rpn_cls: 0.008244  loss_rpn_loc: 0.0102    time: 0.5839  last_time: 0.5211  data_time: 0.0089  last_data_time: 0.0059   lr: 4.4955e-05  max_mem: 2582M\n",
            "[05/12 18:22:45 d2.utils.events]:  eta: 0:12:28  iter: 199  total_loss: 1.99  loss_cls: 0.5778  loss_box_reg: 0.7823  loss_mask: 0.584  loss_rpn_cls: 0.01297  loss_rpn_loc: 0.01205    time: 0.5846  last_time: 0.5861  data_time: 0.0104  last_data_time: 0.0053   lr: 4.995e-05  max_mem: 2582M\n",
            "[05/12 18:22:56 d2.utils.events]:  eta: 0:12:15  iter: 219  total_loss: 2.097  loss_cls: 0.6023  loss_box_reg: 0.8671  loss_mask: 0.5719  loss_rpn_cls: 0.00969  loss_rpn_loc: 0.01019    time: 0.5830  last_time: 0.5806  data_time: 0.0080  last_data_time: 0.0052   lr: 5.4945e-05  max_mem: 2582M\n",
            "[05/12 18:23:08 d2.utils.events]:  eta: 0:12:09  iter: 239  total_loss: 1.674  loss_cls: 0.4756  loss_box_reg: 0.7078  loss_mask: 0.5258  loss_rpn_cls: 0.009234  loss_rpn_loc: 0.007807    time: 0.5837  last_time: 0.4904  data_time: 0.0094  last_data_time: 0.0053   lr: 5.994e-05  max_mem: 2582M\n",
            "[05/12 18:23:20 d2.utils.events]:  eta: 0:11:55  iter: 259  total_loss: 1.715  loss_cls: 0.4711  loss_box_reg: 0.733  loss_mask: 0.4654  loss_rpn_cls: 0.004581  loss_rpn_loc: 0.008217    time: 0.5828  last_time: 0.6155  data_time: 0.0080  last_data_time: 0.0071   lr: 6.4935e-05  max_mem: 2583M\n",
            "[05/12 18:23:31 d2.utils.events]:  eta: 0:11:44  iter: 279  total_loss: 1.834  loss_cls: 0.4864  loss_box_reg: 0.8299  loss_mask: 0.4521  loss_rpn_cls: 0.00377  loss_rpn_loc: 0.009076    time: 0.5826  last_time: 0.6227  data_time: 0.0062  last_data_time: 0.0050   lr: 6.993e-05  max_mem: 2583M\n",
            "[05/12 18:23:43 d2.utils.events]:  eta: 0:11:30  iter: 299  total_loss: 1.612  loss_cls: 0.4248  loss_box_reg: 0.7441  loss_mask: 0.4121  loss_rpn_cls: 0.003582  loss_rpn_loc: 0.007246    time: 0.5817  last_time: 0.5667  data_time: 0.0080  last_data_time: 0.0054   lr: 7.4925e-05  max_mem: 2583M\n",
            "[05/12 18:23:54 d2.utils.events]:  eta: 0:11:18  iter: 319  total_loss: 1.489  loss_cls: 0.4161  loss_box_reg: 0.6667  loss_mask: 0.3987  loss_rpn_cls: 0.004002  loss_rpn_loc: 0.008767    time: 0.5815  last_time: 0.5978  data_time: 0.0088  last_data_time: 0.0056   lr: 7.992e-05  max_mem: 2583M\n",
            "[05/12 18:24:06 d2.utils.events]:  eta: 0:11:07  iter: 339  total_loss: 1.429  loss_cls: 0.3661  loss_box_reg: 0.6896  loss_mask: 0.3342  loss_rpn_cls: 0.003097  loss_rpn_loc: 0.008622    time: 0.5818  last_time: 0.5717  data_time: 0.0100  last_data_time: 0.0053   lr: 8.4915e-05  max_mem: 2583M\n",
            "[05/12 18:24:18 d2.utils.events]:  eta: 0:10:58  iter: 359  total_loss: 1.406  loss_cls: 0.3909  loss_box_reg: 0.675  loss_mask: 0.3246  loss_rpn_cls: 0.001527  loss_rpn_loc: 0.00777    time: 0.5829  last_time: 0.5871  data_time: 0.0088  last_data_time: 0.0053   lr: 8.991e-05  max_mem: 2583M\n",
            "[05/12 18:24:30 d2.utils.events]:  eta: 0:10:46  iter: 379  total_loss: 1.392  loss_cls: 0.3911  loss_box_reg: 0.7091  loss_mask: 0.3031  loss_rpn_cls: 0.0008886  loss_rpn_loc: 0.007666    time: 0.5825  last_time: 0.6545  data_time: 0.0112  last_data_time: 0.0831   lr: 9.4905e-05  max_mem: 2583M\n",
            "[05/12 18:24:41 d2.utils.events]:  eta: 0:10:33  iter: 399  total_loss: 1.286  loss_cls: 0.3419  loss_box_reg: 0.6505  loss_mask: 0.2648  loss_rpn_cls: 0.005328  loss_rpn_loc: 0.009599    time: 0.5815  last_time: 0.5805  data_time: 0.0069  last_data_time: 0.0059   lr: 9.99e-05  max_mem: 2583M\n",
            "[05/12 18:24:52 d2.utils.events]:  eta: 0:10:22  iter: 419  total_loss: 1.024  loss_cls: 0.3325  loss_box_reg: 0.4898  loss_mask: 0.2383  loss_rpn_cls: 0.0002572  loss_rpn_loc: 0.007792    time: 0.5814  last_time: 0.5645  data_time: 0.0081  last_data_time: 0.0054   lr: 0.0001049  max_mem: 2583M\n",
            "[05/12 18:25:04 d2.utils.events]:  eta: 0:10:11  iter: 439  total_loss: 1.115  loss_cls: 0.3355  loss_box_reg: 0.5232  loss_mask: 0.2078  loss_rpn_cls: 0.000741  loss_rpn_loc: 0.01124    time: 0.5815  last_time: 0.5162  data_time: 0.0085  last_data_time: 0.0056   lr: 0.00010989  max_mem: 2583M\n",
            "[05/12 18:25:16 d2.utils.events]:  eta: 0:10:00  iter: 459  total_loss: 1.018  loss_cls: 0.3252  loss_box_reg: 0.4532  loss_mask: 0.192  loss_rpn_cls: 0.001623  loss_rpn_loc: 0.01171    time: 0.5813  last_time: 0.5546  data_time: 0.0104  last_data_time: 0.0053   lr: 0.00011489  max_mem: 2583M\n",
            "[05/12 18:25:27 d2.utils.events]:  eta: 0:09:48  iter: 479  total_loss: 0.8126  loss_cls: 0.283  loss_box_reg: 0.3661  loss_mask: 0.1645  loss_rpn_cls: 0.0002235  loss_rpn_loc: 0.0109    time: 0.5817  last_time: 0.5630  data_time: 0.0114  last_data_time: 0.0055   lr: 0.00011988  max_mem: 2583M\n",
            "[05/12 18:25:39 d2.utils.events]:  eta: 0:09:37  iter: 499  total_loss: 0.8212  loss_cls: 0.2835  loss_box_reg: 0.3881  loss_mask: 0.1809  loss_rpn_cls: 0.001444  loss_rpn_loc: 0.009853    time: 0.5810  last_time: 0.6125  data_time: 0.0064  last_data_time: 0.0173   lr: 0.00012488  max_mem: 2583M\n",
            "[05/12 18:25:50 d2.utils.events]:  eta: 0:09:25  iter: 519  total_loss: 0.7359  loss_cls: 0.2138  loss_box_reg: 0.3291  loss_mask: 0.1729  loss_rpn_cls: 0.0007441  loss_rpn_loc: 0.006289    time: 0.5804  last_time: 0.5643  data_time: 0.0080  last_data_time: 0.0059   lr: 0.00012987  max_mem: 2583M\n",
            "[05/12 18:26:02 d2.utils.events]:  eta: 0:09:14  iter: 539  total_loss: 0.7581  loss_cls: 0.2029  loss_box_reg: 0.3706  loss_mask: 0.1512  loss_rpn_cls: 0.0005743  loss_rpn_loc: 0.009197    time: 0.5804  last_time: 0.5686  data_time: 0.0094  last_data_time: 0.0058   lr: 0.00013487  max_mem: 2583M\n",
            "[05/12 18:26:13 d2.utils.events]:  eta: 0:09:03  iter: 559  total_loss: 0.7233  loss_cls: 0.2467  loss_box_reg: 0.3471  loss_mask: 0.1454  loss_rpn_cls: 0.0008854  loss_rpn_loc: 0.007765    time: 0.5804  last_time: 0.6122  data_time: 0.0099  last_data_time: 0.0054   lr: 0.00013986  max_mem: 2583M\n",
            "[05/12 18:26:25 d2.utils.events]:  eta: 0:08:53  iter: 579  total_loss: 0.7611  loss_cls: 0.2754  loss_box_reg: 0.3167  loss_mask: 0.1644  loss_rpn_cls: 0.0003402  loss_rpn_loc: 0.008744    time: 0.5813  last_time: 0.6671  data_time: 0.0149  last_data_time: 0.0154   lr: 0.00014486  max_mem: 2583M\n",
            "[05/12 18:26:38 d2.utils.events]:  eta: 0:08:42  iter: 599  total_loss: 0.8376  loss_cls: 0.2998  loss_box_reg: 0.3359  loss_mask: 0.1576  loss_rpn_cls: 0.001879  loss_rpn_loc: 0.007591    time: 0.5832  last_time: 0.6818  data_time: 0.0224  last_data_time: 0.0465   lr: 0.00014985  max_mem: 2583M\n",
            "[05/12 18:26:51 d2.utils.events]:  eta: 0:08:30  iter: 619  total_loss: 0.6429  loss_cls: 0.1907  loss_box_reg: 0.3077  loss_mask: 0.1189  loss_rpn_cls: 6.07e-05  loss_rpn_loc: 0.005883    time: 0.5842  last_time: 0.5969  data_time: 0.0169  last_data_time: 0.0129   lr: 0.00015485  max_mem: 2583M\n",
            "[05/12 18:27:02 d2.utils.events]:  eta: 0:08:19  iter: 639  total_loss: 0.7208  loss_cls: 0.2306  loss_box_reg: 0.3085  loss_mask: 0.1273  loss_rpn_cls: 0.0002277  loss_rpn_loc: 0.006118    time: 0.5840  last_time: 0.5570  data_time: 0.0058  last_data_time: 0.0057   lr: 0.00015984  max_mem: 2583M\n",
            "[05/12 18:27:14 d2.utils.events]:  eta: 0:08:07  iter: 659  total_loss: 0.6694  loss_cls: 0.1924  loss_box_reg: 0.3062  loss_mask: 0.1671  loss_rpn_cls: 0.001984  loss_rpn_loc: 0.006151    time: 0.5839  last_time: 0.5617  data_time: 0.0071  last_data_time: 0.0049   lr: 0.00016484  max_mem: 2583M\n",
            "[05/12 18:27:26 d2.utils.events]:  eta: 0:07:56  iter: 679  total_loss: 0.7876  loss_cls: 0.2262  loss_box_reg: 0.32  loss_mask: 0.1544  loss_rpn_cls: 0.0004934  loss_rpn_loc: 0.006414    time: 0.5842  last_time: 0.6037  data_time: 0.0087  last_data_time: 0.0051   lr: 0.00016983  max_mem: 2583M\n",
            "[05/12 18:27:38 d2.utils.events]:  eta: 0:07:44  iter: 699  total_loss: 0.5515  loss_cls: 0.1568  loss_box_reg: 0.2506  loss_mask: 0.1357  loss_rpn_cls: 0.0009939  loss_rpn_loc: 0.00589    time: 0.5844  last_time: 0.5984  data_time: 0.0078  last_data_time: 0.0047   lr: 0.00017483  max_mem: 2583M\n",
            "[05/12 18:27:49 d2.utils.events]:  eta: 0:07:33  iter: 719  total_loss: 0.6982  loss_cls: 0.2085  loss_box_reg: 0.2969  loss_mask: 0.1583  loss_rpn_cls: 0.001561  loss_rpn_loc: 0.007701    time: 0.5844  last_time: 0.5838  data_time: 0.0094  last_data_time: 0.0060   lr: 0.00017982  max_mem: 2583M\n",
            "[05/12 18:28:01 d2.utils.events]:  eta: 0:07:21  iter: 739  total_loss: 0.5431  loss_cls: 0.1589  loss_box_reg: 0.2481  loss_mask: 0.1303  loss_rpn_cls: 0.0001145  loss_rpn_loc: 0.005468    time: 0.5845  last_time: 0.5438  data_time: 0.0113  last_data_time: 0.0052   lr: 0.00018482  max_mem: 2583M\n",
            "[05/12 18:28:13 d2.utils.events]:  eta: 0:07:10  iter: 759  total_loss: 0.6319  loss_cls: 0.1881  loss_box_reg: 0.2729  loss_mask: 0.1535  loss_rpn_cls: 0.001049  loss_rpn_loc: 0.00711    time: 0.5845  last_time: 0.6209  data_time: 0.0086  last_data_time: 0.0261   lr: 0.00018981  max_mem: 2583M\n",
            "[05/12 18:28:24 d2.utils.events]:  eta: 0:06:58  iter: 779  total_loss: 0.7112  loss_cls: 0.1701  loss_box_reg: 0.3299  loss_mask: 0.1653  loss_rpn_cls: 0.0005966  loss_rpn_loc: 0.007669    time: 0.5841  last_time: 0.5975  data_time: 0.0064  last_data_time: 0.0051   lr: 0.00019481  max_mem: 2583M\n",
            "[05/12 18:28:36 d2.utils.events]:  eta: 0:06:46  iter: 799  total_loss: 0.7195  loss_cls: 0.2135  loss_box_reg: 0.2977  loss_mask: 0.1531  loss_rpn_cls: 0.0006001  loss_rpn_loc: 0.007366    time: 0.5841  last_time: 0.5735  data_time: 0.0115  last_data_time: 0.0052   lr: 0.0001998  max_mem: 2583M\n",
            "[05/12 18:28:47 d2.utils.events]:  eta: 0:06:34  iter: 819  total_loss: 0.656  loss_cls: 0.1993  loss_box_reg: 0.298  loss_mask: 0.1589  loss_rpn_cls: 8.285e-05  loss_rpn_loc: 0.004309    time: 0.5838  last_time: 0.5175  data_time: 0.0106  last_data_time: 0.0056   lr: 0.0002048  max_mem: 2583M\n",
            "[05/12 18:28:59 d2.utils.events]:  eta: 0:06:23  iter: 839  total_loss: 0.5607  loss_cls: 0.1554  loss_box_reg: 0.2507  loss_mask: 0.1383  loss_rpn_cls: 0.0001295  loss_rpn_loc: 0.005508    time: 0.5840  last_time: 0.6013  data_time: 0.0091  last_data_time: 0.0053   lr: 0.00020979  max_mem: 2583M\n",
            "[05/12 18:29:11 d2.utils.events]:  eta: 0:06:11  iter: 859  total_loss: 0.5469  loss_cls: 0.1483  loss_box_reg: 0.275  loss_mask: 0.1274  loss_rpn_cls: 0.0004964  loss_rpn_loc: 0.004994    time: 0.5840  last_time: 0.5403  data_time: 0.0097  last_data_time: 0.0062   lr: 0.00021479  max_mem: 2583M\n",
            "[05/12 18:29:22 d2.utils.events]:  eta: 0:06:00  iter: 879  total_loss: 0.477  loss_cls: 0.1202  loss_box_reg: 0.2441  loss_mask: 0.1276  loss_rpn_cls: 9.292e-05  loss_rpn_loc: 0.005366    time: 0.5840  last_time: 0.6714  data_time: 0.0071  last_data_time: 0.0117   lr: 0.00021978  max_mem: 2583M\n",
            "[05/12 18:29:34 d2.utils.events]:  eta: 0:05:48  iter: 899  total_loss: 0.477  loss_cls: 0.1106  loss_box_reg: 0.215  loss_mask: 0.126  loss_rpn_cls: 5.348e-06  loss_rpn_loc: 0.00509    time: 0.5838  last_time: 0.6607  data_time: 0.0073  last_data_time: 0.0048   lr: 0.00022478  max_mem: 2583M\n",
            "[05/12 18:29:45 d2.utils.events]:  eta: 0:05:37  iter: 919  total_loss: 0.4905  loss_cls: 0.1544  loss_box_reg: 0.236  loss_mask: 0.1249  loss_rpn_cls: 0.0001394  loss_rpn_loc: 0.004932    time: 0.5835  last_time: 0.4836  data_time: 0.0074  last_data_time: 0.0056   lr: 0.00022977  max_mem: 2583M\n",
            "[05/12 18:29:57 d2.utils.events]:  eta: 0:05:25  iter: 939  total_loss: 0.5033  loss_cls: 0.1276  loss_box_reg: 0.2076  loss_mask: 0.1408  loss_rpn_cls: 6.699e-05  loss_rpn_loc: 0.004035    time: 0.5835  last_time: 0.5662  data_time: 0.0088  last_data_time: 0.0071   lr: 0.00023477  max_mem: 2583M\n",
            "[05/12 18:30:08 d2.utils.events]:  eta: 0:05:13  iter: 959  total_loss: 0.5843  loss_cls: 0.1736  loss_box_reg: 0.2517  loss_mask: 0.141  loss_rpn_cls: 0.0002576  loss_rpn_loc: 0.005202    time: 0.5832  last_time: 0.5002  data_time: 0.0125  last_data_time: 0.0052   lr: 0.00023976  max_mem: 2583M\n",
            "[05/12 18:30:20 d2.utils.events]:  eta: 0:05:02  iter: 979  total_loss: 0.5042  loss_cls: 0.1553  loss_box_reg: 0.2316  loss_mask: 0.134  loss_rpn_cls: 0.0003959  loss_rpn_loc: 0.005469    time: 0.5832  last_time: 0.5965  data_time: 0.0083  last_data_time: 0.0084   lr: 0.00024476  max_mem: 2583M\n",
            "[05/12 18:30:31 d2.utils.events]:  eta: 0:04:50  iter: 999  total_loss: 0.6244  loss_cls: 0.1651  loss_box_reg: 0.2816  loss_mask: 0.1451  loss_rpn_cls: 0.0006655  loss_rpn_loc: 0.00596    time: 0.5829  last_time: 0.6335  data_time: 0.0069  last_data_time: 0.0123   lr: 0.00024975  max_mem: 2583M\n",
            "[05/12 18:30:43 d2.utils.events]:  eta: 0:04:38  iter: 1019  total_loss: 0.4481  loss_cls: 0.1174  loss_box_reg: 0.199  loss_mask: 0.1495  loss_rpn_cls: 0.0003534  loss_rpn_loc: 0.004357    time: 0.5828  last_time: 0.6327  data_time: 0.0069  last_data_time: 0.0054   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:30:55 d2.utils.events]:  eta: 0:04:27  iter: 1039  total_loss: 0.5168  loss_cls: 0.09754  loss_box_reg: 0.2006  loss_mask: 0.1298  loss_rpn_cls: 8.684e-05  loss_rpn_loc: 0.004606    time: 0.5830  last_time: 0.7064  data_time: 0.0153  last_data_time: 0.0896   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:31:07 d2.utils.events]:  eta: 0:04:15  iter: 1059  total_loss: 0.5192  loss_cls: 0.122  loss_box_reg: 0.2507  loss_mask: 0.1276  loss_rpn_cls: 0.0003317  loss_rpn_loc: 0.00503    time: 0.5834  last_time: 0.5720  data_time: 0.0109  last_data_time: 0.0060   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:31:19 d2.utils.events]:  eta: 0:04:04  iter: 1079  total_loss: 0.5141  loss_cls: 0.1331  loss_box_reg: 0.2294  loss_mask: 0.1422  loss_rpn_cls: 0.0001454  loss_rpn_loc: 0.005485    time: 0.5832  last_time: 0.5594  data_time: 0.0082  last_data_time: 0.0055   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:31:30 d2.utils.events]:  eta: 0:03:53  iter: 1099  total_loss: 0.5685  loss_cls: 0.147  loss_box_reg: 0.2729  loss_mask: 0.1568  loss_rpn_cls: 0.0008314  loss_rpn_loc: 0.006117    time: 0.5833  last_time: 0.5911  data_time: 0.0097  last_data_time: 0.0050   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:31:42 d2.utils.events]:  eta: 0:03:41  iter: 1119  total_loss: 0.4768  loss_cls: 0.1147  loss_box_reg: 0.2197  loss_mask: 0.1163  loss_rpn_cls: 8.106e-05  loss_rpn_loc: 0.005425    time: 0.5836  last_time: 0.6816  data_time: 0.0082  last_data_time: 0.0244   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:31:54 d2.utils.events]:  eta: 0:03:29  iter: 1139  total_loss: 0.5448  loss_cls: 0.109  loss_box_reg: 0.2457  loss_mask: 0.1443  loss_rpn_cls: 6.783e-05  loss_rpn_loc: 0.00392    time: 0.5834  last_time: 0.5706  data_time: 0.0059  last_data_time: 0.0052   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:32:06 d2.utils.events]:  eta: 0:03:18  iter: 1159  total_loss: 0.4753  loss_cls: 0.1154  loss_box_reg: 0.2151  loss_mask: 0.1249  loss_rpn_cls: 0.0001395  loss_rpn_loc: 0.005198    time: 0.5835  last_time: 0.4570  data_time: 0.0098  last_data_time: 0.0053   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:32:17 d2.utils.events]:  eta: 0:03:06  iter: 1179  total_loss: 0.538  loss_cls: 0.1351  loss_box_reg: 0.2291  loss_mask: 0.1433  loss_rpn_cls: 0.0002102  loss_rpn_loc: 0.006235    time: 0.5837  last_time: 0.5679  data_time: 0.0092  last_data_time: 0.0058   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:32:29 d2.utils.events]:  eta: 0:02:55  iter: 1199  total_loss: 0.4548  loss_cls: 0.1264  loss_box_reg: 0.2257  loss_mask: 0.1029  loss_rpn_cls: 8.608e-05  loss_rpn_loc: 0.003821    time: 0.5838  last_time: 0.5536  data_time: 0.0085  last_data_time: 0.0064   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:32:41 d2.utils.events]:  eta: 0:02:43  iter: 1219  total_loss: 0.4515  loss_cls: 0.1206  loss_box_reg: 0.2132  loss_mask: 0.1266  loss_rpn_cls: 0.0001903  loss_rpn_loc: 0.005545    time: 0.5837  last_time: 0.5737  data_time: 0.0093  last_data_time: 0.0063   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:32:52 d2.utils.events]:  eta: 0:02:31  iter: 1239  total_loss: 0.4565  loss_cls: 0.1048  loss_box_reg: 0.2372  loss_mask: 0.1223  loss_rpn_cls: 6.93e-05  loss_rpn_loc: 0.005194    time: 0.5833  last_time: 0.5437  data_time: 0.0070  last_data_time: 0.0121   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:33:04 d2.utils.events]:  eta: 0:02:20  iter: 1259  total_loss: 0.4654  loss_cls: 0.1578  loss_box_reg: 0.1863  loss_mask: 0.107  loss_rpn_cls: 0.0001913  loss_rpn_loc: 0.005007    time: 0.5832  last_time: 0.6082  data_time: 0.0087  last_data_time: 0.0131   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:33:15 d2.utils.events]:  eta: 0:02:08  iter: 1279  total_loss: 0.4987  loss_cls: 0.121  loss_box_reg: 0.2009  loss_mask: 0.1427  loss_rpn_cls: 0.0005112  loss_rpn_loc: 0.004306    time: 0.5833  last_time: 0.5876  data_time: 0.0077  last_data_time: 0.0066   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:33:27 d2.utils.events]:  eta: 0:01:56  iter: 1299  total_loss: 0.4084  loss_cls: 0.1105  loss_box_reg: 0.2103  loss_mask: 0.1039  loss_rpn_cls: 0.0003247  loss_rpn_loc: 0.003868    time: 0.5835  last_time: 0.6100  data_time: 0.0132  last_data_time: 0.0065   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:33:39 d2.utils.events]:  eta: 0:01:45  iter: 1319  total_loss: 0.4339  loss_cls: 0.0753  loss_box_reg: 0.2089  loss_mask: 0.1125  loss_rpn_cls: 6.233e-05  loss_rpn_loc: 0.004278    time: 0.5835  last_time: 0.5781  data_time: 0.0103  last_data_time: 0.0057   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:33:51 d2.utils.events]:  eta: 0:01:33  iter: 1339  total_loss: 0.436  loss_cls: 0.1219  loss_box_reg: 0.2114  loss_mask: 0.1126  loss_rpn_cls: 5.76e-05  loss_rpn_loc: 0.003638    time: 0.5836  last_time: 0.5948  data_time: 0.0111  last_data_time: 0.0059   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:34:03 d2.utils.events]:  eta: 0:01:21  iter: 1359  total_loss: 0.4285  loss_cls: 0.08108  loss_box_reg: 0.2247  loss_mask: 0.1202  loss_rpn_cls: 7.066e-05  loss_rpn_loc: 0.003735    time: 0.5838  last_time: 0.6246  data_time: 0.0079  last_data_time: 0.0092   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:34:14 d2.utils.events]:  eta: 0:01:10  iter: 1379  total_loss: 0.5454  loss_cls: 0.1437  loss_box_reg: 0.2247  loss_mask: 0.1497  loss_rpn_cls: 0.0002664  loss_rpn_loc: 0.005082    time: 0.5835  last_time: 0.6543  data_time: 0.0065  last_data_time: 0.0049   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:34:26 d2.utils.events]:  eta: 0:00:58  iter: 1399  total_loss: 0.4563  loss_cls: 0.08643  loss_box_reg: 0.2189  loss_mask: 0.1162  loss_rpn_cls: 8.836e-05  loss_rpn_loc: 0.004986    time: 0.5836  last_time: 0.5277  data_time: 0.0079  last_data_time: 0.0060   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:34:38 d2.utils.events]:  eta: 0:00:46  iter: 1419  total_loss: 0.4112  loss_cls: 0.09246  loss_box_reg: 0.2006  loss_mask: 0.1081  loss_rpn_cls: 0.0001025  loss_rpn_loc: 0.003908    time: 0.5836  last_time: 0.5724  data_time: 0.0101  last_data_time: 0.0058   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:34:49 d2.utils.events]:  eta: 0:00:35  iter: 1439  total_loss: 0.3766  loss_cls: 0.07054  loss_box_reg: 0.1799  loss_mask: 0.1173  loss_rpn_cls: 5.723e-05  loss_rpn_loc: 0.004656    time: 0.5837  last_time: 0.5858  data_time: 0.0099  last_data_time: 0.0055   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:35:01 d2.utils.events]:  eta: 0:00:23  iter: 1459  total_loss: 0.4883  loss_cls: 0.1019  loss_box_reg: 0.2441  loss_mask: 0.1392  loss_rpn_cls: 0.0002216  loss_rpn_loc: 0.004415    time: 0.5837  last_time: 0.5790  data_time: 0.0086  last_data_time: 0.0053   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:35:13 d2.utils.events]:  eta: 0:00:11  iter: 1479  total_loss: 0.3689  loss_cls: 0.06946  loss_box_reg: 0.1698  loss_mask: 0.107  loss_rpn_cls: 1.965e-05  loss_rpn_loc: 0.003671    time: 0.5837  last_time: 0.5696  data_time: 0.0080  last_data_time: 0.0059   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:35:26 d2.utils.events]:  eta: 0:00:00  iter: 1499  total_loss: 0.4562  loss_cls: 0.1007  loss_box_reg: 0.2224  loss_mask: 0.1182  loss_rpn_cls: 0.0001547  loss_rpn_loc: 0.005405    time: 0.5834  last_time: 0.5848  data_time: 0.0060  last_data_time: 0.0054   lr: 0.00025  max_mem: 2583M\n",
            "[05/12 18:35:26 d2.engine.hooks]: Overall training speed: 1498 iterations in 0:14:33 (0.5834 s / it)\n",
            "[05/12 18:35:26 d2.engine.hooks]: Total training time: 0:14:36 (0:00:02 on hooks)\n",
            "[05/12 18:35:42 d2.data.build]: Distribution of instances among all 4 categories:\n",
            "|  category  | #instances   |  category  | #instances   |  category   | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:-----------:|:-------------|\n",
            "|    good    | 64           | exc_solder | 58           | poor_solder | 14           |\n",
            "|   spike    | 39           |            |              |             |              |\n",
            "|   total    | 175          |            |              |             |              |\n",
            "[05/12 18:35:42 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/12 18:35:42 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:35:42 d2.data.common]: Serializing 102 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:35:42 d2.data.common]: Serialized dataset takes 0.12 MiB\n",
            "WARNING [05/12 18:35:42 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "WARNING [05/12 18:35:43 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/12 18:35:43 d2.evaluation.coco_evaluation]: Trying to convert 'soldef2_val' to COCO format ...\n",
            "[05/12 18:35:43 d2.data.datasets.coco]: Converting annotations of dataset 'soldef2_val' to COCO format ...)\n",
            "[05/12 18:35:43 d2.data.datasets.coco]: Converting dataset dicts into COCO format\n",
            "[05/12 18:35:43 d2.data.datasets.coco]: Conversion finished, #images: 102, #annotations: 175\n",
            "[05/12 18:35:43 d2.data.datasets.coco]: Caching COCO format annotations at '/content/drive/MyDrive/output_dataset_2/soldef2_val_coco_format.json' ...\n",
            "[05/12 18:35:43 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/12 18:35:43 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/12 18:35:43 d2.data.common]: Serializing 102 elements to byte tensors and concatenating them all ...\n",
            "[05/12 18:35:43 d2.data.common]: Serialized dataset takes 0.12 MiB\n",
            "[05/12 18:35:43 d2.evaluation.evaluator]: Start inference on 102 batches\n",
            "[05/12 18:35:48 d2.evaluation.evaluator]: Inference done 11/102. Dataloading: 0.0393 s/iter. Inference: 0.1247 s/iter. Eval: 0.0210 s/iter. Total: 0.1850 s/iter. ETA=0:00:16\n",
            "[05/12 18:35:53 d2.evaluation.evaluator]: Inference done 28/102. Dataloading: 0.1198 s/iter. Inference: 0.1227 s/iter. Eval: 0.0338 s/iter. Total: 0.2766 s/iter. ETA=0:00:20\n",
            "[05/12 18:35:58 d2.evaluation.evaluator]: Inference done 44/102. Dataloading: 0.0733 s/iter. Inference: 0.1270 s/iter. Eval: 0.0918 s/iter. Total: 0.2924 s/iter. ETA=0:00:16\n",
            "[05/12 18:36:04 d2.evaluation.evaluator]: Inference done 60/102. Dataloading: 0.0552 s/iter. Inference: 0.1296 s/iter. Eval: 0.1222 s/iter. Total: 0.3072 s/iter. ETA=0:00:12\n",
            "[05/12 18:36:09 d2.evaluation.evaluator]: Inference done 80/102. Dataloading: 0.0491 s/iter. Inference: 0.1329 s/iter. Eval: 0.1106 s/iter. Total: 0.2928 s/iter. ETA=0:00:06\n",
            "[05/12 18:36:14 d2.evaluation.evaluator]: Inference done 98/102. Dataloading: 0.0401 s/iter. Inference: 0.1331 s/iter. Eval: 0.1182 s/iter. Total: 0.2916 s/iter. ETA=0:00:01\n",
            "[05/12 18:36:15 d2.evaluation.evaluator]: Total inference time: 0:00:28.078296 (0.289467 s / iter per device, on 1 devices)\n",
            "[05/12 18:36:15 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:12 (0.132836 s / iter per device, on 1 devices)\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Saving results to /content/drive/MyDrive/output_dataset_2/coco_instances_results.json\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.693\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.934\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.789\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.694\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.683\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.796\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 69.345 | 93.406 | 78.903 |  nan  | 50.000 | 69.447 |\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     | category    | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|:------------|:-------|\n",
            "| good       | 86.021 | exc_solder | 79.485 | poor_solder | 61.348 |\n",
            "| spike      | 50.525 |            |        |             |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.05 seconds.\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/12 18:36:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.691\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.932\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.786\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.692\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.666\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.780\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.782\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 69.097 | 93.150 | 78.594 |  nan  | 50.000 | 69.208 |\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[05/12 18:36:15 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category   | AP     | category    | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|:------------|:-------|\n",
            "| good       | 87.706 | exc_solder | 79.946 | poor_solder | 56.037 |\n",
            "| spike      | 52.699 |            |        |             |        |\n",
            "\n",
            "========== mAP Evaluation for soldef2 ==========\n",
            "OrderedDict([('bbox', {'AP': 69.34491643048929, 'AP50': 93.40633975594527, 'AP75': 78.90330199358493, 'APs': nan, 'APm': 50.0, 'APl': 69.44669203373002, 'AP-good': 86.02113196610257, 'AP-exc_solder': 79.48497211509432, 'AP-poor_solder': 61.348259825982595, 'AP-spike': 50.5253018147777}), ('segm', {'AP': 69.09710434870382, 'AP50': 93.15020683037297, 'AP75': 78.59447052325531, 'APs': nan, 'APm': 50.0, 'APl': 69.20758322956628, 'AP-good': 87.70604091467462, 'AP-exc_solder': 79.94602977816385, 'AP-poor_solder': 56.03716621662166, 'AP-spike': 52.69918048535518})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install torch==1.8.0 torchvision==0.9.0\n",
        "!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19uhmsOvGvyA",
        "outputId": "ba7cf90a-0bd3-4c03-b2a1-a16409fafe6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.0 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-3y2zmka5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-3y2zmka5\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 536dc9d527074e3b15df5f6677ffe1f4e104a4ab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.8)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6438300 sha256=7d0f69907fdc14f72a6bd1819ef17183bd94e97d04f85f36df3d43174c1c14d1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qi9h4wjh/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=c7f569a1e257cad4ce81a2c7635e3d40fef9a54c356b8f1fccf79ff2e1b2848e\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import torch\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.data import DatasetMapper\n",
        "\n",
        "setup_logger()\n",
        "random.seed(42)\n",
        "\n",
        "# === STEP 1: Dataset Preparation ===\n",
        "\n",
        "base_folder = \"/content/drive/MyDrive/Colab Notebooks/Labeled\"\n",
        "output_base_dataset_1 = \"/content/drive/MyDrive/output_dataset_1\"\n",
        "output_base_dataset_2 = \"/content/drive/MyDrive/output_dataset_2\"\n",
        "\n",
        "class_mapping_dataset_1 = {\"good\": 0, \"no_good\": 1}\n",
        "class_mapping_dataset_2 = {\"good\": 0, \"exc_solder\": 1, \"poor_solder\": 2, \"spike\": 3}\n",
        "\n",
        "for base in [output_base_dataset_1, output_base_dataset_2]:\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(base, \"images\", split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base, \"labels\", split), exist_ok=True)\n",
        "\n",
        "train_ratio = 0.8\n",
        "json_files = [f for f in os.listdir(base_folder) if f.endswith(\".json\")]\n",
        "train_files = random.sample(json_files, int(len(json_files) * train_ratio))\n",
        "val_files = [f for f in json_files if f not in train_files]\n",
        "\n",
        "def is_valid_for_dataset(json_path, class_mapping):\n",
        "    with open(json_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    for shape in data['shapes']:\n",
        "        if shape['label'] not in class_mapping:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "for file_set, split in [(train_files, \"train\"), (val_files, \"val\")]:\n",
        "    for filename in file_set:\n",
        "        json_path = os.path.join(base_folder, filename)\n",
        "        image_name = os.path.splitext(filename)[0] + \".jpg\"\n",
        "        image_path = os.path.join(base_folder, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        # Dataset 1\n",
        "        if is_valid_for_dataset(json_path, class_mapping_dataset_1):\n",
        "            shutil.copy(image_path, os.path.join(output_base_dataset_1, \"images\", split, image_name))\n",
        "            shutil.copy(json_path, os.path.join(output_base_dataset_1, \"labels\", split, filename))\n",
        "\n",
        "        # Dataset 2\n",
        "        if is_valid_for_dataset(json_path, class_mapping_dataset_2):\n",
        "            shutil.copy(image_path, os.path.join(output_base_dataset_2, \"images\", split, image_name))\n",
        "            shutil.copy(json_path, os.path.join(output_base_dataset_2, \"labels\", split, filename))\n",
        "\n",
        "# === STEP 2: Dataset Registration ===\n",
        "\n",
        "def load_soldef_detectron2(img_dir, label_dir, class_names):\n",
        "    dataset_dicts = []\n",
        "    img_files = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
        "\n",
        "    for idx, img_file in enumerate(img_files):\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        json_name = os.path.splitext(img_file)[0] + \".json\"\n",
        "        json_path = os.path.join(label_dir, json_name)\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "            continue\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        record = {\n",
        "            \"file_name\": img_path,\n",
        "            \"image_id\": idx,\n",
        "            \"height\": data.get(\"imageHeight\"),\n",
        "            \"width\": data.get(\"imageWidth\"),\n",
        "            \"annotations\": []\n",
        "        }\n",
        "\n",
        "        for shape in data.get(\"shapes\", []):\n",
        "            label = shape[\"label\"]\n",
        "            if label not in class_names:\n",
        "                continue\n",
        "            pts = shape[\"points\"]\n",
        "            poly = [float(p) for x, y in pts for p in (x, y)]\n",
        "            xs = [p[0] for p in pts]\n",
        "            ys = [p[1] for p in pts]\n",
        "            bbox = [min(xs), min(ys), max(xs), max(ys)]\n",
        "\n",
        "            record[\"annotations\"].append({\n",
        "                \"bbox\": bbox,\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": class_names.index(label)\n",
        "            })\n",
        "\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "def register_soldef_dataset(dataset_name, base_path, class_names):\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        img_dir = os.path.join(base_path, \"images\", split)\n",
        "        label_dir = os.path.join(base_path, \"labels\", split)\n",
        "        name = f\"{dataset_name}_{split}\"\n",
        "        DatasetCatalog.register(name, lambda img=img_dir, lab=label_dir, cls=class_names:\n",
        "                                load_soldef_detectron2(img, lab, cls))\n",
        "        MetadataCatalog.get(name).set(thing_classes=class_names)\n",
        "\n",
        "register_soldef_dataset(\"soldef1\", output_base_dataset_1, [\"good\", \"no_good\"])\n",
        "register_soldef_dataset(\"soldef2\", output_base_dataset_2, [\"good\", \"exc_solder\", \"poor_solder\", \"spike\"])\n",
        "\n",
        "# === STEP 3: Albumentations Mapper ===\n",
        "\n",
        "class AlbumentationsMapper(DatasetMapper):\n",
        "    def __init__(self, cfg, is_train=True):\n",
        "        super().__init__(cfg, is_train)\n",
        "        self.augmentations = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.4),\n",
        "            A.MotionBlur(p=0.2),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        dataset_dict = dataset_dict.copy()\n",
        "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "        annos = dataset_dict[\"annotations\"]\n",
        "\n",
        "        bboxes = [anno[\"bbox\"] for anno in annos]\n",
        "        category_ids = [anno[\"category_id\"] for anno in annos]\n",
        "\n",
        "        transformed = self.augmentations(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "        image = transformed[\"image\"]\n",
        "        annos = transformed[\"bboxes\"]\n",
        "        cats = transformed[\"category_ids\"]\n",
        "\n",
        "        dataset_dict[\"image\"] = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
        "        new_annos = []\n",
        "        for bbox, category_id in zip(annos, cats):\n",
        "            x0, y0, x1, y1 = bbox\n",
        "            new_annos.append({\n",
        "                \"bbox\": [x0, y0, x1, y1],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"category_id\": category_id,\n",
        "            })\n",
        "        dataset_dict[\"instances\"] = utils.annotations_to_instances(new_annos, image.shape[:2])\n",
        "        return dataset_dict\n",
        "\n",
        "# === STEP 4: Custom Trainer with Albumentations ===\n",
        "\n",
        "class TrainerWithAlbumentations(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=AlbumentationsMapper(cfg, is_train=True))\n",
        "\n",
        "# === STEP 5: Training + Evaluation ===\n",
        "\n",
        "def train_detectron2(dataset_name, num_classes, output_dir, max_iter=1500):\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.DATASETS.TRAIN = (f\"{dataset_name}_train\",)\n",
        "    cfg.DATASETS.TEST = (f\"{dataset_name}_val\",)\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER = max_iter\n",
        "    cfg.SOLVER.STEPS = []\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "    cfg.OUTPUT_DIR = output_dir\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    trainer = TrainerWithAlbumentations(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "\n",
        "    # mAP Evaluation\n",
        "    evaluator = COCOEvaluator(f\"{dataset_name}_val\", cfg, False, output_dir=output_dir)\n",
        "    val_loader = build_detection_test_loader(cfg, f\"{dataset_name}_val\")\n",
        "    metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "    print(f\"\\n========== mAP Evaluation for {dataset_name} ==========\")\n",
        "    print(metrics)\n",
        "\n",
        "# === STEP 6: Train Both Datasets ===\n",
        "\n",
        "train_detectron2(\"soldef1\", num_classes=2, output_dir=\"/content/drive/MyDrive/output_dataset_1\")\n",
        "train_detectron2(\"soldef2\", num_classes=4, output_dir=\"/content/drive/MyDrive/output_dataset_2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Br6BlrlSv53j",
        "outputId": "a8058e97-39c8-48cc-b826-0c2ea9276d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 05:39:41 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[05/13 05:39:41 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/13 05:40:10 d2.data.build]: Removed 0 images with no usable annotations. 227 images left.\n",
            "[05/13 05:40:10 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|    good    | 117          |  no_good   | 115          |\n",
            "|            |              |            |              |\n",
            "|   total    | 232          |            |              |\n",
            "[05/13 05:40:10 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/13 05:40:10 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/13 05:40:10 d2.data.common]: Serializing 227 elements to byte tensors and concatenating them all ...\n",
            "[05/13 05:40:10 d2.data.common]: Serialized dataset takes 0.16 MiB\n",
            "[05/13 05:40:10 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[05/13 05:40:10 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:01, 131MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 05:40:12 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7534384d1617>:151: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  dataset_dict[\"image\"] = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
            "<ipython-input-3-7534384d1617>:151: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  dataset_dict[\"image\"] = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR [05/13 05:40:15 d2.engine.train_loop]: Exception during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/engine/train_loop.py\", line 155, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/engine/defaults.py\", line 530, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/engine/train_loop.py\", line 310, in run_step\n",
            "    loss_dict = self.model(data)\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/modeling/meta_arch/rcnn.py\", line 167, in forward\n",
            "    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\", line 743, in forward\n",
            "    losses.update(self._forward_mask(features, proposals))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\", line 846, in _forward_mask\n",
            "    return self.mask_head(features, instances)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/mask_head.py\", line 199, in forward\n",
            "    return {\"loss_mask\": mask_rcnn_loss(x, instances, self.vis_period) * self.loss_weight}\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/mask_head.py\", line 65, in mask_rcnn_loss\n",
            "    gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/detectron2/structures/instances.py\", line 66, in __getattr__\n",
            "    raise AttributeError(\"Cannot find field '{}' in the given Instances!\".format(name))\n",
            "AttributeError: Cannot find field 'gt_masks' in the given Instances!\n",
            "[05/13 05:40:15 d2.engine.hooks]: Total training time: 0:00:03 (0:00:00 on hooks)\n",
            "[05/13 05:40:15 d2.utils.events]:  iter: 0       lr: N/A  max_mem: 5441M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Cannot find field 'gt_masks' in the given Instances!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7534384d1617>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# === STEP 6: Train Both Datasets ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mtrain_detectron2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soldef1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/output_dataset_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0mtrain_detectron2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soldef2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/output_dataset_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7534384d1617>\u001b[0m in \u001b[0;36mtrain_detectron2\u001b[0;34m(dataset_name, num_classes, output_dir, max_iter)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerWithAlbumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# mAP Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \"\"\"\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             assert hasattr(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdo\u001b[0m \u001b[0msomething\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_period\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_event_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;31m# heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;31m# predicted by the box head.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_keypoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\u001b[0m in \u001b[0;36m_forward_mask\u001b[0;34m(self, features, instances)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_in_features\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_keypoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/mask_head.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, instances)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask_rcnn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_period\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mmask_rcnn_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/modeling/roi_heads/mask_head.py\u001b[0m in \u001b[0;36mmask_rcnn_loss\u001b[0;34m(pred_mask_logits, instances, vis_period)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mgt_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_classes_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0minstances_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_side_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         ).to(device=pred_mask_logits.device)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/structures/instances.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_fields\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot find field '{}' in the given Instances!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Cannot find field 'gt_masks' in the given Instances!"
          ]
        }
      ]
    }
  ]
}